---
title: "Scratch Work"
author: "Simon Dovan Nguyen"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    theme: lumen
    css: assets/styles.css
---

# Set Up
## Notes

## Libr.
```{r message=FALSE, warning=FALSE}
library(MASS)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(class)
library(glmnet)
```

## Rm. Var.
```{r}
rm(list=ls())
```

## Functions
```{r}
dir = "/Users/simondn/Documents/RashomonActiveLearning/"
source(paste0(dir,"GenerateData.R"))
source(paste0(dir,"RandomStart.R"))
source(paste0(dir,"RandomSelector.R"))
source(paste0(dir,"StoppingCriteria.R"))
```

# Generate Data

## Data
```{r}
set.seed(1)
### Input ###
N = 1000
K = 5
NClass = 7
ClassProportion = c(rep(1/14,6),8/14)
MeanMatrix = rep(0,K)
CorrVal = 0

### Data ###
DGPResults = GenerateDataFunc(N, K, NClass, ClassProportion, MeanMatrix, CorrVal)
dat = DGPResults$dat
TrueBetas = DGPResults$TrueBetas
Noise = DGPResults$noise
# rm(DGPResults)
```

## Visualize
```{r}
HistogramY = ggplot(data = dat) +
  geom_histogram(mapping = aes(x = Y, color = as.factor(Label)))
  # geom_point(mapping = aes(x = Y, y = 0, color = as.factor(Label))) 

ScatterPlotY = ggplot(data = dat) +
  geom_point(mapping = aes(x = Y, y = 0, color = as.factor(Label)))


ScatterPlotX1X2 = ggplot(data = dat) +
  geom_point(mapping = aes(x = X1, y = X2, color = as.factor(Label))) 

ScatterPlotY
HistogramY
ScatterPlotX1X2
```

# Model
## KNN
```{r}
KNNModel = knn(train = dat[, setdiff(names(dat), c("ID", "Y"))],
               test =  dat[, setdiff(names(dat), c("ID", "Y"))],
               cl = dat[,"Label"],
               k = 5)
mean(KNNModel != dat[,"Label"])
```


## Logistic
```{r}
# LogisticModel = glm(as.factor(Label) ~ ., 
#                     data = dat[, setdiff(names(dat), c("ID", "Y"))],
#                     family = "binomial")
# 
# LogisticModelPredicted = predict(LogisticModel,
#                                  newdata = dat[, setdiff(names(dat), c("ID", "Y"))],
#                                  type = "response")
# LogisticModelPredicted <- ifelse(LogisticModelPredicted > 0.5,1,0)+1
# 
# mean(LogisticModelPredicted != dat$Label)
```


## LASSO
```{r}
# ### Lambda ###
# CVLasso = cv.glmnet(x=as.matrix(dat[, setdiff(names(dat), c("ID", "Label", "Y"))]), 
#                     y=as.matrix(dat[, "Label"]), 
#                     alpha = 1,
#                     type.measure = "class",
#                     family = "binomial")
# LASSOLambda = CVLasso$lambda.min
# 
# # Training Prediction #
# LASSOTrainingPrediction = glmnet(x=as.matrix(dat[, setdiff(names(dat), c("ID", "Label", "Y"))]), 
#                                  y=as.matrix(dat[, "Label"]), 
#                                  alpha = 1,
#                                  lambda = LASSOLambda,
#                                  family = "binomial")
# 
# LASSOTrainingPrediction = predict(LASSOTrainingPrediction, 
#                                   newx = as.matrix(dat[, setdiff(names(dat), c("ID", "Label", "Y"))]), 
#                                   s = LASSOLambda,
#                                   type = "response")[,1]
# LASSOTrainingPrediction =  ifelse(LASSOTrainingPrediction > 0.5,1,0)
# 
# mean((LASSOTrainingPrediction+1) != dat$Label)
```

## Random Selection
```{r}
RandomStart = RandomStartFunc(InitialN=1, dat=dat)
TrainingSet = RandomStart$TrainingSet
CandidateSet = RandomStart$CandidateSet
```

## Simulation

```{r}
RandomSelectorSimulationFunc = function(dat,
                                        TailN,
                                        ErrorThreshold,
                                        VarThreshold){
  
  
  ### Progres Bar ###
  pb = txtProgressBar(min = 0, 
                      max = N,
                      style = 3,  
                      width = 50,
                      char = "=")
  start_time = Sys.time()
  
  ### Simulation ###
  Error = numeric(N)
  TailN = 3
  StopIter = NULL
  for(iter in 1:N){
    
    # Progress Bar #
    setTxtProgressBar(pb, iter)
  
    KnnModelIter = class::knn(train = TrainingSet[, setdiff(names(dat), c("Y","ID"))], 
                              test = TrainingSet[,setdiff(names(dat), c("Y","ID"))], 
                              cl = TrainingSet[,"Label"], 
                              k = 3)
    Error[iter] = mean(KnnModelIter != TrainingSet[,"Label"])
    
    
    if(iter > TailN){if(is.null(StopIter)){StopIter = StoppingCriteriaFunc(ErrorVector = Error[1:iter], 
                                                                           ErrorThreshold = 0.05, 
                                                                           VarThreshold = 0.01, 
                                                                           TailN = 10)}}
    
    RandomSelector = RandomSelectorFunc(SelectorN = 1, TrainingSet, CandidateSet)
    TrainingSet = RandomSelector$TrainingSet
    CandidateSet = RandomSelector$CandidateSet
  }
  ### System Time ###
  close(pb)
  end_time = Sys.time()
  run_time = end_time - start_time
  
  ErrorScatterPlot = ggplot() +
    # geom_point(mapping = aes(x = 1:length(Error), y = Error)) + 
    geom_line(mapping = aes(x = 1:length(Error), y = Error)) + 
    geom_vline(xintercept = StopIter, color = "red") + 
    geom_hline(yintercept = Error[StopIter], color = "black", linetype = "dotted", alpha = 0.4) + 
    annotate("text", x = StopIter, y = max(Error), label = StopIter) + 
    annotate("text", x = 0, y = Error[StopIter], label = round(Error[StopIter],3))
  
  ErrorScatterPlot
  
  return(list(Error = Error,
              StopIter = StopIter,
              ErrorScatterPlot = ErrorScatterPlot,
              run_time = run_time))
    
  }
```

```{r}
RandomSelectorSimulationFunc(dat = dat,
                             TailN = 5,
                             ErrorThreshold = 0.01,
                             VarThreshold = .1) -> SimResults
```

```{r}
SimResults$ErrorScatterPlot
```



















